{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa152f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a376c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(\"../datasets/radnet_synthetic_nodes.csv\", index_col=0)\n",
    "edges = pd.read_csv(\"../datasets/radnet_synthetic_edges.csv\")\n",
    "\n",
    "# tensores\n",
    "x = torch.tensor(nodes.values, dtype=torch.float)\n",
    "edge_index = torch.tensor([edges[\"src\"].values, edges[\"dst\"].values], dtype=torch.long)\n",
    "\n",
    "label_map = {\"baixa\": 0, \"media\": 1, \"alta\": 2}\n",
    "y = torch.tensor([label_map[s] for s in edges[\"label\"].values], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "858d78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3        # fração para teste\n",
    "random_state = 42      # semente para reprodutibilidade\n",
    "\n",
    "all_idx = np.arange(edge_index.size(1))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    all_idx,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_idx  = torch.tensor(test_idx,  dtype=torch.long)\n",
    "\n",
    "train_edges = edge_index[:, train_idx]\n",
    "test_edges  = edge_index[:, test_idx]\n",
    "train_labels = y[train_idx]\n",
    "test_labels  = y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90806a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        h = torch.matmul(adj, x)   # agregação\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = torch.matmul(adj, h)\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_channels * 2, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x_i, x_j):\n",
    "        z = torch.cat([x_i, x_j], dim=-1)\n",
    "        z = F.relu(self.lin1(z))\n",
    "        return self.lin2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a267fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = x.size(0)\n",
    "adj = torch.eye(num_nodes)\n",
    "for i, j in zip(edges[\"src\"], edges[\"dst\"]):\n",
    "    adj[i, j] = 1\n",
    "\n",
    "# normalização\n",
    "deg = adj.sum(1)\n",
    "deg_inv = torch.pow(deg, -0.5)\n",
    "deg_inv[deg_inv == float(\"inf\")] = 0\n",
    "D_inv = torch.diag(deg_inv)\n",
    "adj_norm = D_inv @ adj @ D_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f9a1d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | Loss: 1.0724 | Test Acc: 0.4000 | Test F1: 0.1905\n",
      "Epoch 020 | Loss: 0.8943 | Test Acc: 0.2000 | Test F1: 0.1333\n",
      "Epoch 030 | Loss: 0.7152 | Test Acc: 0.4000 | Test F1: 0.2222\n",
      "Epoch 040 | Loss: 0.5252 | Test Acc: 0.2000 | Test F1: 0.2222\n",
      "Epoch 050 | Loss: 0.3366 | Test Acc: 0.2000 | Test F1: 0.2222\n",
      "Epoch 060 | Loss: 0.1489 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 070 | Loss: 0.0395 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 080 | Loss: 0.0098 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 090 | Loss: 0.0037 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 100 | Loss: 0.0020 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 110 | Loss: 0.0014 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 120 | Loss: 0.0012 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 130 | Loss: 0.0010 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 140 | Loss: 0.0009 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 150 | Loss: 0.0008 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 160 | Loss: 0.0007 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 170 | Loss: 0.0006 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 180 | Loss: 0.0006 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 190 | Loss: 0.0005 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 200 | Loss: 0.0005 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 210 | Loss: 0.0004 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 220 | Loss: 0.0004 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 230 | Loss: 0.0004 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 240 | Loss: 0.0004 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 250 | Loss: 0.0003 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 260 | Loss: 0.0003 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 270 | Loss: 0.0003 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 280 | Loss: 0.0003 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 290 | Loss: 0.0003 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 300 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 310 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 320 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 330 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 340 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 350 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 360 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 370 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 380 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 390 | Loss: 0.0002 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 400 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 410 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 420 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 430 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 440 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 450 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 460 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 470 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 480 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n",
      "Epoch 490 | Loss: 0.0001 | Test Acc: 0.4000 | Test F1: 0.3556\n"
     ]
    }
   ],
   "source": [
    "gcn = SimpleGCN(in_channels=x.size(1), hidden_channels=8, out_channels=4)\n",
    "predictor = LinkPredictor(in_channels=4, hidden_channels=8, num_classes=3)\n",
    "optimizer = torch.optim.Adam(list(gcn.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "    # treino\n",
    "    gcn.train(); predictor.train()\n",
    "    z = gcn(x, adj_norm)\n",
    "    preds = []\n",
    "    for i, j in zip(train_edges[0], train_edges[1]):\n",
    "        preds.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    \n",
    "    loss = F.cross_entropy(preds, train_labels)\n",
    "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "    \n",
    "    # teste periódico\n",
    "    if epoch % 10 == 0:\n",
    "        gcn.eval(); predictor.eval()\n",
    "        z = gcn(x, adj_norm)\n",
    "        preds = []\n",
    "        for i, j in zip(test_edges[0], test_edges[1]):\n",
    "            preds.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        \n",
    "        y_true = test_labels.numpy()\n",
    "        y_pred = preds.argmax(dim=-1).detach().numpy()\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        \n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f} | Test F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac615a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo em ../models/gcn_state.json\n",
      "Modelo salvo em ../models/predictor_state.json\n",
      "Modelo salvo em ../models/meta.json\n",
      "Modelo salvo em ../models/graph_artifacts.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "ckpt_gcn = \"../models/gcn_state.pt\"\n",
    "ckpt_pred = \"../models/predictor_state.pt\"\n",
    "ckpt_meta = \"../models/meta.pt\"\n",
    "ckpt_graph = \"../models/graph_artifacts.pt\"\n",
    "\n",
    "# Salva pesos\n",
    "torch.save(gcn.state_dict(), ckpt_gcn)\n",
    "torch.save(predictor.state_dict(), ckpt_pred)\n",
    "\n",
    "# Salva metadados\n",
    "torch.save({\n",
    "    \"num_features\": x.size(1),   # numero de features do nó (aqui 3)\n",
    "    \"gcn_hidden\": 8,             # tamanho da hidden layer do GCN\n",
    "    \"gcn_out\": 4,                # saída do GCN\n",
    "    \"pred_hidden\": 8,            # hidden layer do LinkPredictor\n",
    "    \"num_classes\": 3             # numero de classes (baixa, media, alta)\n",
    "}, ckpt_meta)\n",
    "\n",
    "# Salva artefatos do grafo\n",
    "torch.save({\n",
    "    \"x\": x.cpu(),              # features dos nós\n",
    "    \"adj_norm\": adj_norm.cpu() # adjacência normalizada\n",
    "}, ckpt_graph)\n",
    "\n",
    "print(\"Modelo salvo em ../models/gcn_state.json\")\n",
    "print(\"Modelo salvo em ../models/predictor_state.json\")\n",
    "print(\"Modelo salvo em ../models/meta.json\")\n",
    "print(\"Modelo salvo em ../models/graph_artifacts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1746dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas salvas em ../results/result.json\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"baixa\", \"media\", \"alta\"]  # nomes de classes\n",
    "\n",
    "gcn.eval(); predictor.eval()\n",
    "with torch.no_grad():\n",
    "    # embeddings dos nós\n",
    "    z = gcn(x, adj_norm)\n",
    "\n",
    "    def eval_split(edges_tensor, labels_tensor):\n",
    "        # logits para cada aresta do split\n",
    "        logits = []\n",
    "        for i, j in zip(edges_tensor[0], edges_tensor[1]):\n",
    "            logits.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "        logits = torch.cat(logits, dim=0)\n",
    "\n",
    "        y_true = labels_tensor.cpu().numpy()\n",
    "        y_pred = logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        # métricas agregadas\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec_macro  = precision_score(y_true, y_pred, average=\"macro\",  zero_division=0)\n",
    "        rec_macro   = recall_score(y_true,    y_pred, average=\"macro\",  zero_division=0)\n",
    "        f1_macro    = f1_score(y_true,        y_pred, average=\"macro\",  zero_division=0)\n",
    "\n",
    "        prec_weight = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "        rec_weight  = recall_score(y_true,    y_pred, average=\"weighted\", zero_division=0)\n",
    "        f1_weight   = f1_score(y_true,        y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        # métricas por classe (mesma ordem de class_names)\n",
    "        prec_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        rec_per_class  = recall_score(y_true,    y_pred, average=None, zero_division=0)\n",
    "        f1_per_class   = f1_score(y_true,        y_pred, average=None, zero_division=0)\n",
    "\n",
    "        per_class = {\n",
    "            cname: {\n",
    "                \"precision\": float(prec_per_class[idx]),\n",
    "                \"recall\":    float(rec_per_class[idx]),\n",
    "                \"f1\":        float(f1_per_class[idx]),\n",
    "            }\n",
    "            for idx, cname in enumerate(class_names)\n",
    "        }\n",
    "\n",
    "        aggregate = {\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision_macro\":  float(prec_macro),\n",
    "            \"recall_macro\":     float(rec_macro),\n",
    "            \"f1_macro\":         float(f1_macro),\n",
    "            \"precision_weighted\": float(prec_weight),\n",
    "            \"recall_weighted\":    float(rec_weight),\n",
    "            \"f1_weighted\":        float(f1_weight),\n",
    "        }\n",
    "\n",
    "        return aggregate, per_class\n",
    "\n",
    "    train_agg, train_per_class = eval_split(train_edges, train_labels)\n",
    "    test_agg,  test_per_class  = eval_split(test_edges,  test_labels)\n",
    "\n",
    "# Monta dicionário e salva em JSON\n",
    "results = {\n",
    "    \"task\": \"edge_load_classification\",\n",
    "    \"classes\": class_names,\n",
    "    \"train\": {\n",
    "        **train_agg,\n",
    "        \"per_class\": train_per_class,\n",
    "    },\n",
    "    \"test\": {\n",
    "        **test_agg,\n",
    "        \"per_class\": test_per_class,\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "with open(\"../results/result.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Métricas salvas em ../results/result.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
